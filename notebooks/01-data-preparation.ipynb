{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "In this notebook, first processing of the data from [OMNIWeb](https://omniweb.gsfc.nasa.gov/) is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/u0124144/Documents/DTW_measure\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is read by the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(filepath='data/raw/features.csv'):\n",
    "    return pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "raw = read_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data as if contains a year, day of year and hour column (since the low-resolution data was used). This is used to create a timestamp index for the data with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n",
    "                 seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n",
    "    '''Create timestamp index from raw data\n",
    "    '''\n",
    "    years = np.asarray(years) - 1970\n",
    "    months = np.asarray(months) - 1\n",
    "    days = np.asarray(days) - 1\n",
    "    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n",
    "             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n",
    "    vals = (years, months, days, weeks, hours, minutes, seconds,\n",
    "            milliseconds, microseconds, nanoseconds)\n",
    "    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n",
    "               if v is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some new features are added that are computed from the already available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "     data['Delta_Dst'] = data.Dst.diff()\n",
    "     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this is combined into one single function that returns us the first processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_scl</th>\n",
       "      <th>|B|</th>\n",
       "      <th>B_Lat</th>\n",
       "      <th>B_Lng</th>\n",
       "      <th>Bx</th>\n",
       "      <th>By_GSE</th>\n",
       "      <th>Bz_GSE</th>\n",
       "      <th>By_GSM</th>\n",
       "      <th>Bz_GSM</th>\n",
       "      <th>|RMS|</th>\n",
       "      <th>...</th>\n",
       "      <th>Pflux30</th>\n",
       "      <th>Pflux60</th>\n",
       "      <th>Flux_F</th>\n",
       "      <th>ap</th>\n",
       "      <th>f10.7</th>\n",
       "      <th>AL</th>\n",
       "      <th>AU</th>\n",
       "      <th>Magn_M</th>\n",
       "      <th>Lyman_alpha</th>\n",
       "      <th>Delta_Dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01 00:00:00</th>\n",
       "      <td>8.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>35.5</td>\n",
       "      <td>347.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>189.7</td>\n",
       "      <td>-14</td>\n",
       "      <td>29</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 01:00:00</th>\n",
       "      <td>8.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-28.7</td>\n",
       "      <td>319.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>189.7</td>\n",
       "      <td>-17</td>\n",
       "      <td>13</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 02:00:00</th>\n",
       "      <td>8.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-41.1</td>\n",
       "      <td>314.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>189.7</td>\n",
       "      <td>-44</td>\n",
       "      <td>48</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 03:00:00</th>\n",
       "      <td>8.4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>-31.8</td>\n",
       "      <td>308.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>189.7</td>\n",
       "      <td>-199</td>\n",
       "      <td>112</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 04:00:00</th>\n",
       "      <td>8.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>299.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-6.8</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>189.7</td>\n",
       "      <td>-29</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     B_scl  |B|  B_Lat  B_Lng   Bx  By_GSE  Bz_GSE  By_GSM  \\\n",
       "1981-01-01 00:00:00    8.1  7.6   35.5  347.8  6.1    -1.3     4.4    -2.5   \n",
       "1981-01-01 01:00:00    8.7  8.6  -28.7  319.8  5.8    -4.9    -4.1    -3.7   \n",
       "1981-01-01 02:00:00    8.7  8.5  -41.1  314.4  4.5    -4.6    -5.6    -3.3   \n",
       "1981-01-01 03:00:00    8.4  7.8  -31.8  308.0  4.1    -5.2    -4.1    -4.5   \n",
       "1981-01-01 04:00:00    8.2  7.9   -5.6  299.0  3.8    -6.9    -0.8    -6.8   \n",
       "\n",
       "                     Bz_GSM  |RMS|  ...  Pflux30  Pflux60  Flux_F  ap  f10.7  \\\n",
       "1981-01-01 00:00:00     3.9    0.1  ...      NaN      NaN       0   7  189.7   \n",
       "1981-01-01 01:00:00    -5.2    0.2  ...      NaN      NaN       0   7  189.7   \n",
       "1981-01-01 02:00:00    -6.4    0.2  ...      NaN      NaN       0   7  189.7   \n",
       "1981-01-01 03:00:00    -4.9    0.2  ...      NaN      NaN       0  27  189.7   \n",
       "1981-01-01 04:00:00    -1.5    0.2  ...      NaN      NaN       0  27  189.7   \n",
       "\n",
       "                      AL   AU  Magn_M  Lyman_alpha  Delta_Dst  \n",
       "1981-01-01 00:00:00  -14   29     2.8     0.009516        NaN  \n",
       "1981-01-01 01:00:00  -17   13     2.4     0.009516        2.0  \n",
       "1981-01-01 02:00:00  -44   48     2.8     0.009516        2.0  \n",
       "1981-01-01 03:00:00 -199  112     3.1     0.009516       -2.0  \n",
       "1981-01-01 04:00:00  -29  139     NaN     0.009516        5.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    '''Preprocess data\n",
    "    '''\n",
    "    data.index = compose_date(data['YEAR'], days=data['DOY'], hours=data['Hour'])\n",
    "    data = data.drop(columns=['YEAR', 'DOY', 'Hour'])\n",
    "    data = data.drop(columns=['pc', 'Pflux1', 'Pflux2', 'Pflux4'])\n",
    "    data = add_features(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "processed = preprocess(raw)\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.to_hdf('data/interim/data.h5', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaNn values are not yet removed, and will be done when the data is prepared for input into the LSTM. This is to ensure all the values are continuous in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for input\n",
    "\n",
    "From the data the features needed for input are extracted and transformed in the correct input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/interim/data2.h5 does not exist\n"
     ]
    }
   ],
   "source": [
    "# Features for the experiment\n",
    "features = ['Dst', '|B|', 'Bz_GSM', 'SWDens', 'SWSpeed']\n",
    "output = 'Dst'\n",
    "startdate = '14-01-2001'\n",
    "enddate = '01-01-2016'\n",
    "time_back = 6\n",
    "time_forward = 6\n",
    "\n",
    "\n",
    "data = pd.read_hdf('data/interim/data.h5', 'data')\n",
    "data = data[startdate:enddate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract training and test data\n",
    "\n",
    "First the data is split in training, validation and test set. The test set is explicitly set to the month July and December of each year. The validation set are randomly chosen months from the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_split(split, train_ind, test_ind):\n",
    "    merged = None\n",
    "    tr_ind = split.copy()\n",
    "    te_ind = split.copy()\n",
    "    train_ind.sort()\n",
    "    test_ind.sort()\n",
    "    for ind in test_ind[::-1]:\n",
    "        del tr_ind[ind]\n",
    "    for ind in train_ind[::-1]:\n",
    "        del te_ind[ind]\n",
    "    return pd.concat(tr_ind).sort_index(), pd.concat(te_ind).sort_index()\n",
    "\n",
    "def controlled_train_test_split(data):\n",
    "    '''Input:\n",
    "        data: panda dataframe with dates\n",
    "       Output:\n",
    "        test set: The month July and December of each year\n",
    "        train set: The remaining months\n",
    "    '''\n",
    "    split = [g for n,g in data.groupby(pd.Grouper(freq='M')) if g.shape[0] != 0]\n",
    "\n",
    "    test_ind = np.hstack([[3+i*12, 7+i*12, 11+i*12] for i in range(int(len(split)/12))])\n",
    "    train_ind = list(filter(lambda x: x not in test_ind, np.arange(len(split))))\n",
    "\n",
    "    train, test = extract_from_split(split, train_ind, test_ind)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def split_data(data, test_size, freq='M'):\n",
    "    '''Input:\n",
    "        input_, output_: numpy matrices that need to be randomly split on the first index\n",
    "        test_size, train_size, valid_size: percentages that sum to 1\n",
    "    '''\n",
    "    split = [g for n,g in data.groupby(pd.Grouper(freq=freq)) if g.shape[0] != 0]\n",
    "    random = 10321\n",
    "    train_ind, test_ind = train_test_split(np.arange(len(split)), random_state=random, test_size=test_size)\n",
    "    train, test = extract_from_split(split, train_ind, test_ind)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def extract_data(data, features, output):\n",
    "    if type(features) is not list:\n",
    "        features = [features]\n",
    "    if type(output) is not list:\n",
    "        output = [output]\n",
    "\n",
    "    data_in = data[features].copy()\n",
    "    data_out = data[output].shift(-1).copy()\n",
    "    return data_in, data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = controlled_train_test_split(data)\n",
    "train, valid = split_data(train, 0.2, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, train_out = extract_data(train, features, output)\n",
    "valid_in, valid_out = extract_data(valid, features, output)\n",
    "test_in, test_out = extract_data(test, features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "The training, test and validation set has to be normalized first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def shift_and_normalize(data, scaler=None):\n",
    "    errors = data[data.isna().any(axis=1)]\n",
    "    clean = data.dropna(axis=0)\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(clean.values)\n",
    "    scaled_values = scaler.transform(clean.values)\n",
    "    clean = pd.DataFrame(data=scaled_values, index=clean.index, columns=clean.columns)\n",
    "    return pd.concat([clean, errors]).sort_index(), scaler\n",
    "\n",
    "def preprocess_data(data, scaler=None):\n",
    "    if scaler is None:\n",
    "        data, scaler = shift_and_normalize(data)\n",
    "    else:\n",
    "        data, _ = shift_and_normalize(data, scaler)\n",
    "\n",
    "    return data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_in, sclr = preprocess_data(train_in)\n",
    "val_in, _ = preprocess_data(valid_in, sclr)\n",
    "t_in, _ = preprocess_data(test_in, sclr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfrom to suitable input\n",
    "Finally, the data set is processed to be used as input in the LSTM. Here, also invalid measures are filtered from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_lstm_input(input_, output_, time_back, time_forward):\n",
    "    num_examples = input_.shape[0]\n",
    "    size = num_examples - time_forward - time_back + 1\n",
    "    num_features = input_.shape[1]\n",
    "    lookup = np.zeros((size, time_forward), dtype='datetime64[s]')\n",
    "    X = np.zeros((size, time_back, num_features))\n",
    "    y = np.zeros((size, output_.shape[1], time_forward))\n",
    "    valid_ins = input_.iloc[:,0].rolling(str(time_back)+'h').apply(lambda x: True if x.shape[0] == time_back else False, raw=True)\n",
    "    dates = input_.index.values\n",
    "    input_ = input_.values\n",
    "    output_ = output_.values\n",
    "    ind = 0\n",
    "    for i, val in valid_ins.reset_index(drop=True).iteritems():\n",
    "        if val != 1:\n",
    "            continue\n",
    "        j = i + 1\n",
    "        X[ind] = input_[j-time_back:j, :]\n",
    "        out = output_[i:i+time_forward, :].T                \n",
    "        if out.shape[1] != time_forward:\n",
    "            continue\n",
    "        y[ind] = out\n",
    "        lookup[ind] = dates[i:i+time_forward] # Store outputdates\n",
    "        if not np.isnan(X[ind]).any():\n",
    "            if not np.isnan(y[ind]).any():\n",
    "                ind += 1\n",
    "\n",
    "    return X[:ind], y[:ind], lookup[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, train_out, _ = format_to_lstm_input(tr_in, train_out, time_back, time_forward)\n",
    "valid_in, valid_out, _ = format_to_lstm_input(val_in, valid_out, time_back, time_forward)\n",
    "test_in, test_out, lookup = format_to_lstm_input(t_in, test_out, time_back, time_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing storm data \n",
    "Now we extract the storms that occur in the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of storms in dataset: 17\n"
     ]
    }
   ],
   "source": [
    "def get_storm_dates(stormname='data/external/semi_auto_storms_8114.csv'):\n",
    "    return pd.read_csv(stormname, index_col=0, dtype={'date1': str, 'date2': str},\n",
    "                       parse_dates=['date1', 'date2'])\n",
    "\n",
    "def get_storms(data, storm_dates):\n",
    "    '''Check which storms lie in the given dataset\n",
    "    Input:\n",
    "        data: dataset with dates\n",
    "        storm_dates: list of known storm occurences\n",
    "    Output:\n",
    "        measured features during the storm\n",
    "        storm-dates that lie in data\n",
    "    '''\n",
    "    rs = []\n",
    "    valid_storms = []\n",
    "    for (_, dates) in storm_dates.iterrows():\n",
    "        ss = data.loc[dates[0]:dates[1]]\n",
    "        if ss.shape[0] != 0:\n",
    "            rs.append(ss)\n",
    "            valid_storms.append((dates[0].strftime('%Y-%m-%d'), dates[1].strftime('%Y-%m-%d')))\n",
    "    print('Number of storms in dataset: {}'.format(len(valid_storms)))\n",
    "    return pd.concat(rs), np.array(valid_storms)\n",
    "\n",
    "storm_dates = get_storm_dates()\n",
    "test_storms, test_storm_dates = get_storms(test, storm_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def store_data_sets(train_in, train_out, valid_in, valid_out, test_in, test_out, lookup, test_storm_dates, fname='data/processed/datasets.h5'):\n",
    "    with h5py.File(fname, 'w') as f:\n",
    "        train = f.create_group('train_sets')\n",
    "        train.create_dataset('train_in', data=train_in)\n",
    "        train.create_dataset('train_out', data=train_out)\n",
    "        valid = f.create_group('valid_sets')\n",
    "        valid.create_dataset('valid_in', data=valid_in)\n",
    "        valid.create_dataset('valid_out', data=valid_out)\n",
    "        test = f.create_group('test_sets')\n",
    "        test.create_dataset('test_in', data=test_in)\n",
    "        test.create_dataset('test_out', data=test_out)\n",
    "        test.create_dataset('lookup', data=lookup.astype(np.long), dtype='int64')\n",
    "        storms = test.create_group('storms')\n",
    "        storms.create_dataset('storm_dates', data=test_storm_dates.astype('S'))\n",
    "    \n",
    "store_data_sets(train_in, train_out, valid_in, valid_out, test_in, test_out, lookup, test_storm_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtw_measure",
   "language": "python",
   "name": "dtw_measure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
